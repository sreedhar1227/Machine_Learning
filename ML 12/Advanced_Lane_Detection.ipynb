{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline of this project:\n",
    "\n",
    "1) Compute the camera caliberation matrix and distortion coefficient from chessboard images. <br/>\n",
    "2) Apply distortion correction to raw images. <br/>\n",
    "3) Use color gradient to create binary threshholded image. <br/>\n",
    "4) Apply perspective transform to binary threshholded image to get top view. <br/>\n",
    "5) Detect pixel lane and fit to find lane boundary. <br/>\n",
    "6) Determine lane curvature and vehicle position wrt centre. <br/>\n",
    "7) Warp the detected boundaries back to original image. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Caliberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_IMGS = \"camera_cal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_files = os.listdir(CAL_IMGS)\n",
    "assert(len(calib_files) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_imgs(lst, rows, cols=2, figsize=(10, 25), dosave= False, save_dir=\"\"):\n",
    "    assert(len(lst) > 0)\n",
    "    assert(rows > 0)\n",
    "    if dosave:\n",
    "        assert(os.path.exists(save_dir))\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.tight_layout()\n",
    "    for i in range(1, rows * cols +1):\n",
    "        fig.add_subplot(rows, cols, i)\n",
    "        img = mpimg.imread(CAL_IMGS + \"/\"+calib_files[i-1])\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    if dosave:\n",
    "        fig.savefig(save_dir + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save output directory\n",
    "OUTDIR = \"output_images/\"\n",
    "create_dir(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking the image\n",
    "draw_imgs(calib_files, len(calib_files)//2, dosave=True, save_dir=OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caliberation\n",
    "\n",
    "As can be seen in above images there are 9 corners in rows and 6 corners in columns. Lets go ahead and find corners.<br/>\n",
    "There are 3 images for which corners = 9 * 6 doesn't work. But 17 images are enough for caliberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 9\n",
    "ny = 6\n",
    "\n",
    "objp = np.zeros((ny * nx, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[:nx, :ny].T.reshape(-1, 2)\n",
    "\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "failed =[]\n",
    "\n",
    "for idx, name in enumerate(calib_files):\n",
    "    img = cv2.imread(CAL_IMGS + \"/\"+ name)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "    \n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    \n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        \n",
    "        corners = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "        \n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(cv2.cvtColor(cv2.imread(CAL_IMGS + \"/\"+ name), cv2.COLOR_BGR2RGB))\n",
    "        ax1.set_title(\"Original:: \" + name, fontsize=18)\n",
    "        ax2.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title(\"Corners:: \"+ name, fontsize=18)\n",
    "        f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")\n",
    "        \n",
    "    else:\n",
    "        failed.append(name)\n",
    "        \n",
    "print(\"Failed for images: [\")\n",
    "print(failed)\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distortion correction\n",
    "\n",
    "Using object and image points calculated in step 1 to caliberate the camera and compute the camera matrix and distortion coefficients.<br/>\n",
    "Then use these camera matrix and distortion cofficients to undistort images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(img_name, objpoints, imgpoints):\n",
    "    img = cv2.imread(img_name)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1:], None, None)\n",
    "    undist= cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_no_read(img, objpoints, imgpoints):\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1:], None, None)\n",
    "    undist= cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undist = undistort(CAL_IMGS+\"/calibration10.jpg\", objpoints, imgpoints)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(cv2.imread(CAL_IMGS+\"/calibration10.jpg\"), cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: calibration10.jpg\" , fontsize=18)\n",
    "ax2.imshow(cv2.cvtColor(undist,cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title(\"Undistorted:: calibration10.jpg\", fontsize=18)\n",
    "f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob('test_images/test*.jpg')\n",
    "for image in images:\n",
    "    undist = undistort(image, objpoints, imgpoints)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "    ax2.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\n",
    "    ax2.set_title(\"Undistorted:: \"+ image, fontsize=18)\n",
    "    f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient and color transform\n",
    "\n",
    "\n",
    "We'll use sobel filter in both x and y direction to get gradient change in both axes to generate binary threshhold image. <br/>\n",
    "We'll also use color space HLS to get color transformed binary threshold image. <br/>\n",
    "We'll combine both these outputs to get final binary threshold image.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_thresh(img, sobel_kernel=3, mag_thresh=(0,255), return_grad= False, direction ='x'):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    grad = None\n",
    "    scaled_sobel = None\n",
    "    \n",
    "    # Sobel x\n",
    "    if direction.lower() == 'x':\n",
    "        grad = cv2.Sobel(gray, cv2.CV_64F, 1, 0,ksize=sobel_kernel) # Take the derivative in x       \n",
    "    # Sobel y\n",
    "    else:\n",
    "        grad = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel) # Take the derivative in y\n",
    "        \n",
    "    if return_grad == True:\n",
    "        return grad\n",
    "        \n",
    "    abs_sobel = np.absolute(grad) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel < mag_thresh[1])] = 1\n",
    "    \n",
    "    return grad_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = abs_thresh(img, sobel_kernel=3, mag_thresh=(30, 100), direction='x')\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = abs_thresh(img, sobel_kernel=3, mag_thresh=(30, 120), direction='y')\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_threshold(img, sobel_kernel=3, mag_thresh=(0, 255)):    \n",
    "    xgrad =  abs_thresh(img, sobel_kernel=sobel_kernel, mag_thresh=mag_thresh, return_grad=True)\n",
    "    ygrad =  abs_thresh(img, sobel_kernel=sobel_kernel, mag_thresh=mag_thresh, return_grad=True, direction='y')\n",
    "    \n",
    "    magnitude = np.sqrt(np.square(xgrad)+np.square(ygrad))\n",
    "    abs_magnitude = np.absolute(magnitude)\n",
    "    scaled_magnitude = np.uint8(255*abs_magnitude/np.max(abs_magnitude))\n",
    "    mag_binary = np.zeros_like(scaled_magnitude)\n",
    "    mag_binary[(scaled_magnitude >= mag_thresh[0]) & (scaled_magnitude < mag_thresh[1])] = 1\n",
    "    \n",
    "    return mag_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = mag_threshold(img, mag_thresh=(30, 100))\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    xgrad =  cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    ygrad =  cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    xabs = np.absolute(xgrad)\n",
    "    yabs = np.absolute(ygrad)\n",
    "    \n",
    "    grad_dir = np.arctan2(yabs, xabs)\n",
    "    \n",
    "    binary_output = np.zeros_like(grad_dir).astype(np.uint8)\n",
    "    binary_output[(grad_dir >= thresh[0]) & (grad_dir < thresh[1])] = 1\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb_thresh_img(img, channel='R', thresh=(0, 255)):\n",
    "    img1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if channel == 'R':\n",
    "        bin_img = img1[:, :, 0]\n",
    "    if channel == 'G' :\n",
    "        bin_img = img1[:, :, 1]\n",
    "    if channel == 'B' :\n",
    "        bin_img = img1[:, :, 2]\n",
    "        \n",
    "    binary_img = np.zeros_like(bin_img).astype(np.uint8) \n",
    "    binary_img[(bin_img >= thresh[0]) & (bin_img < thresh[1])] = 1\n",
    "    \n",
    "    return binary_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_rgb_thresh_img(img, thresh=(230, 255))\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_rgb_thresh_img(img, thresh=(200, 255), channel='G')\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_rgb_thresh_img(img, thresh=(185, 255), channel='B')\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hls_lthresh_img(img, thresh=(0, 255)):\n",
    "    hls_img= cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    L = hls_img[:, :, 1]\n",
    "\n",
    "    binary_output = np.zeros_like(L).astype(np.uint8)    \n",
    "    binary_output[(L >= thresh[0]) & (L < thresh[1])] = 1\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_hls_lthresh_img(img, thresh=(201, 255))\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hls_sthresh_img(img, thresh=(0, 255)):\n",
    "    hls_img= cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    S = hls_img[:, :, 2]\n",
    "\n",
    "    binary_output = np.zeros_like(S).astype(np.uint8)    \n",
    "    binary_output[(S >= thresh[0]) & (S < thresh[1])] = 1\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_hls_sthresh_img(img, thresh=(150, 255))\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lab_athresh_img(img, thresh=(0,255)):\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    A = lab_img[:, :, 1]\n",
    "    \n",
    "    bin_op = np.zeros_like(A).astype(np.uint8)\n",
    "    bin_op[(A >= thresh[0]) & (A < thresh[1])] = 1\n",
    "    \n",
    "    return bin_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lab_bthresh_img(img, thresh=(0,255)):\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    B = lab_img[:, :, 2]\n",
    "    \n",
    "    bin_op = np.zeros_like(B).astype(np.uint8)\n",
    "    bin_op[(B >= thresh[0]) & (B < thresh[1])] = 1\n",
    "    \n",
    "    return bin_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = undistort(images[0], objpoints, imgpoints)\n",
    "    \n",
    "combined_binary = get_lab_bthresh_img(img, thresh=(147, 255))\n",
    "warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, offset=300)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "f.tight_layout()\n",
    "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "ax2.imshow(warped, cmap='gray')\n",
    "ax2.set_title(\"Transformed:: \"+ image, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_img(img, kernel_size=3, sobel_dirn='X', sobel_thresh=(0,255), r_thresh=(0, 255), \n",
    "                s_thresh=(0,255), b_thresh=(0, 255), g_thresh=(0, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS).astype(np.float32)\n",
    "    h_channel = hls[:,:,0]\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "      \n",
    "    if sobel_dirn == 'X':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = kernel_size)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = kernel_size)\n",
    "        \n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    sbinary = np.zeros_like(scaled_sobel)\n",
    "    sbinary[(scaled_sobel >= sobel_thresh[0]) & (scaled_sobel <= sobel_thresh[1])] = 1\n",
    "    \n",
    "    combined = np.zeros_like(sbinary)\n",
    "    combined[(sbinary == 1)] = 1\n",
    "\n",
    "    # Threshold R color channel\n",
    "    r_binary = get_rgb_thresh_img(img, thresh= r_thresh)\n",
    "    \n",
    "    # Threshhold G color channel\n",
    "    g_binary = get_rgb_thresh_img(img, thresh= g_thresh, channel='G')\n",
    "    \n",
    "    # Threshhold B in LAB\n",
    "    b_binary = get_lab_bthresh_img(img, thresh=b_thresh)\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = get_hls_sthresh_img(img, thresh=s_thresh)\n",
    "\n",
    "    # If two of the three are activated, activate in the binary image\n",
    "    combined_binary = np.zeros_like(combined)\n",
    "    combined_binary[(r_binary == 1) | (combined == 1) | (s_binary == 1)| (b_binary == 1) | (g_binary == 1)] = 1\n",
    "\n",
    "    return combined_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the threshholding\n",
    "kernel_size = 5\n",
    "mag_thresh = (30, 100)\n",
    "r_thresh = (235, 255)\n",
    "s_thresh = (165, 255)\n",
    "b_thresh = (160, 255)\n",
    "g_thresh = (210, 255)\n",
    "\n",
    "for image_name in images:\n",
    "    img = undistort(image_name, objpoints, imgpoints)\n",
    "    \n",
    "    combined_binary = get_bin_img(img, kernel_size=kernel_size, sobel_thresh=mag_thresh, r_thresh=r_thresh, \n",
    "                                  s_thresh=s_thresh, b_thresh = b_thresh, g_thresh=g_thresh)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "    ax2.imshow(combined_binary, cmap='gray')\n",
    "    ax2.set_title(\"Threshold Binary:: \"+ image, fontsize=18)\n",
    "    f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective Transform\n",
    "\n",
    "Perspective transform maps the points in given image to different perspective. <br/>\n",
    "We are here looking for bird's eye view of the road <br/>\n",
    "This will be helpful in finding lane curvature. <br/>\n",
    "Note that after perspective transform the lanes should apear aproximately parallel <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img, offset=250, src=None, dst=None):    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    out_img_orig = np.copy(img)\n",
    "       \n",
    "    leftupper  = (585, 460)\n",
    "    rightupper = (705, 460)\n",
    "    leftlower  = (210, img.shape[0])\n",
    "    rightlower = (1080, img.shape[0])\n",
    "    \n",
    "    \n",
    "    warped_leftupper = (offset,0)\n",
    "    warped_rightupper = (offset, img.shape[0])\n",
    "    warped_leftlower = (img.shape[1] - offset, 0)\n",
    "    warped_rightlower = (img.shape[1] - offset, img.shape[0])\n",
    "    \n",
    "    color_r = [0, 0, 255]\n",
    "    color_g = [0, 255, 0]\n",
    "    line_width = 5\n",
    "    \n",
    "    if src is not None:\n",
    "        src = src\n",
    "    else:\n",
    "        src = np.float32([leftupper, leftlower, rightupper, rightlower])\n",
    "        \n",
    "    if dst is not None:\n",
    "        dst = dst\n",
    "    else:\n",
    "        dst = np.float32([warped_leftupper, warped_rightupper, warped_leftlower, warped_rightlower])\n",
    "    \n",
    "    cv2.line(out_img_orig, leftlower, leftupper, color_r, line_width)\n",
    "    cv2.line(out_img_orig, leftlower, rightlower, color_r , line_width * 2)\n",
    "    cv2.line(out_img_orig, rightupper, rightlower, color_r, line_width)\n",
    "    cv2.line(out_img_orig, rightupper, leftupper, color_g, line_width)\n",
    "    \n",
    "    # calculate the perspective transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    # Warp the image\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.WARP_FILL_OUTLIERS+cv2.INTER_CUBIC)\n",
    "    out_warped_img = np.copy(warped)\n",
    "    \n",
    "    cv2.line(out_warped_img, warped_rightupper, warped_leftupper, color_r, line_width)\n",
    "    cv2.line(out_warped_img, warped_rightupper, warped_rightlower, color_r , line_width * 2)\n",
    "    cv2.line(out_warped_img, warped_leftlower, warped_rightlower, color_r, line_width)\n",
    "    cv2.line(out_warped_img, warped_leftlower, warped_leftupper, color_g, line_width)\n",
    "    \n",
    "    return warped, M, minv, out_img_orig, out_warped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    img = cv2.imread(image)\n",
    "    warped, M, minv, out_img_orig, out_warped_img = transform_image(img)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(cv2.cvtColor(out_img_orig, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "    ax2.imshow(cv2.cvtColor(out_warped_img, cv2.COLOR_BGR2RGB))\n",
    "    ax2.set_title(\"Warped:: \"+ image, fontsize=18)\n",
    "    f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    img = undistort(image, objpoints, imgpoints)\n",
    "    combined_binary = get_bin_img(img, kernel_size=kernel_size, sobel_thresh=mag_thresh, \n",
    "                                  r_thresh=r_thresh, s_thresh=s_thresh, b_thresh = b_thresh, g_thresh=g_thresh)\n",
    "    warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary)\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "    ax2.imshow(warped, cmap='gray')\n",
    "    ax2.set_title(\"Transformed:: \"+ image, fontsize=18)\n",
    "    f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane line pixel detection and polynomial fitting\n",
    "\n",
    "With binary image where lane lines are clearly visible, now we have to decide lane pixels <br/>\n",
    "Also we need to decide pixels from left lane and pixels from right lane. <br/>\n",
    "<br/>\n",
    "The threshold image pixels are either 0 or 1, so if we take histogram of the image <br/>\n",
    "the 2 peaks that we might see in histogram might be good position to start to find lane pixels <br/>\n",
    "We can then use sliding window to find further pixels<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lines(warped_img, nwindows=9, margin=80, minpix=40):\n",
    "    \n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(warped_img[warped_img.shape[0]//2:,:], axis=0)\n",
    "        \n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((warped_img, warped_img, warped_img)) * 255\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(warped_img.shape[0]//nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = warped_img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = warped_img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = warped_img.shape[0] - window*window_height\n",
    "        \n",
    "        ### Find the four below boundaries of the window ###\n",
    "        win_xleft_low = leftx_current - margin  \n",
    "        win_xleft_high = leftx_current + margin  \n",
    "        win_xright_low =  rightx_current - margin \n",
    "        win_xright_high = rightx_current + margin  \n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low), (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low), (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        ### Identify the nonzero pixels in x and y within the window ###\n",
    "        good_left_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) &\\\n",
    "                            (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) &\\\n",
    "                            (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        ### If you found > minpix pixels, recenter next window ###\n",
    "        ### (`right` or `leftx_current`) on their mean position ###\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    return leftx, lefty, rightx, righty, left_lane_inds, right_lane_inds, out_img\n",
    "\n",
    "def fit_polynomial(binary_warped, nwindows=9, margin=100, minpix=50, show=True):\n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, left_lane_inds, right_lane_inds, out_img \\\n",
    "        = find_lines(binary_warped, nwindows=nwindows, margin=margin, minpix=minpix)\n",
    "\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    try:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1*ploty**2 + 1*ploty\n",
    "        right_fitx = 1*ploty**2 + 1*ploty\n",
    "\n",
    "    # Colors in the left and right lane regions\n",
    "    out_img[lefty, leftx] = [255, 0, 0]\n",
    "    out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    # Plots the left and right polynomials on the lane lines\n",
    "    if show == True:\n",
    "        plt.plot(left_fitx, ploty, color='yellow')\n",
    "        plt.plot(right_fitx, ploty, color='yellow')\n",
    "\n",
    "    return left_fit, right_fit, left_fitx, right_fitx, left_lane_inds, right_lane_inds, out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip the sliding windows step once you've found the lines\n",
    "\n",
    "Once lines are found, we don't need to do blind search , but we can search around existing line with some margin. As the lanes are not going to shift much between 2 frames of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_around_poly(binary_warped, left_fit, right_fit, ymtr_per_pixel, xmtr_per_pixel, margin=80):\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "                    left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "                    left_fit[1]*nonzeroy + left_fit[2] + margin)))\n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "                    right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "                    right_fit[1]*nonzeroy + right_fit[2] + margin)))\n",
    "    \n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "    # Fit a second order polynomial to each using `np.polyfit`\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Fit second order polynomial to for for points on real world   \n",
    "    left_lane_indices = np.polyfit(lefty*ymtr_per_pixel, leftx*xmtr_per_pixel, 2)\n",
    "    right_lane_indices = np.polyfit(righty*ymtr_per_pixel, rightx*xmtr_per_pixel, 2)\n",
    "    \n",
    "    return left_fit, right_fit, left_lane_indices, right_lane_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_fit, right_fit, left_fitx, right_fitx, left_lane_indices, right_lane_indices, out_img = fit_polynomial(warped, nwindows=20)\n",
    "plt.imshow(out_img)\n",
    "plt.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radius of curvature\n",
    "\n",
    "We can fit the circle that can approximately fits the nearby points locally <br/>\n",
    "\n",
    "![alt text](radius_curvature1.png)\n",
    "\n",
    "The radius of curvature is radius of the circle that fits the curve<br/>\n",
    "The radius of curvature can be found out using equation: <br/>\n",
    "<br/>\n",
    "![alt text](eq1.gif)\n",
    "<br/>\n",
    "For polynomial below are the equation: <br/>\n",
    "![alt text](eq2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius_curvature(img, left_fit, right_fit, xmtr_per_pixel, ymtr_per_pixel):\n",
    "    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    left_fit_cr = np.polyfit(ploty*ymtr_per_pixel, left_fitx*xmtr_per_pixel, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ymtr_per_pixel, right_fitx*xmtr_per_pixel, 2)\n",
    "    \n",
    "    # find radii of curvature\n",
    "    left_rad = ((1 + (2*left_fit_cr[0]*y_eval*ymtr_per_pixel + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_rad = ((1 + (2*right_fit_cr[0]*y_eval*ymtr_per_pixel + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    return (left_rad, right_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_center(img, left_fit, right_fit, xmtr_per_pixel, ymtr_per_pixel):\n",
    "    ## Image mid horizontal position \n",
    "    #xmax = img.shape[1]*xmtr_per_pixel\n",
    "    ymax = img.shape[0]*ymtr_per_pixel\n",
    "    \n",
    "    center = img.shape[1] / 2\n",
    "    \n",
    "    lineLeft = left_fit[0]*ymax**2 + left_fit[1]*ymax + left_fit[2]\n",
    "    lineRight = right_fit[0]*ymax**2 + right_fit[1]*ymax + right_fit[2]\n",
    "    \n",
    "    mid = lineLeft + (lineRight - lineLeft)/2\n",
    "    dist = (mid - center) * xmtr_per_pixel\n",
    "    if dist >= 0. :\n",
    "        message = 'Vehicle location: {:.2f} m right'.format(dist)\n",
    "    else:\n",
    "        message = 'Vehicle location: {:.2f} m left'.format(abs(dist))\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img, left_fit, right_fit, minv):\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "    color_warp = np.zeros_like(img).astype(np.uint8)\n",
    "    \n",
    "    # Find left and right points.\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    \n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "    \n",
    "    # Warp the blank back to original image space using inverse perspective matrix \n",
    "    unwarp_img = cv2.warpPerspective(color_warp, minv, (img.shape[1], img.shape[0]), flags=cv2.WARP_FILL_OUTLIERS+cv2.INTER_CUBIC)\n",
    "    return cv2.addWeighted(img, 1, unwarp_img, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_curvatures(img, leftx, rightx, xmtr_per_pixel, ymtr_per_pixel):\n",
    "    (left_curvature, right_curvature) = radius_curvature(img, leftx, rightx, xmtr_per_pixel, ymtr_per_pixel)\n",
    "    dist_txt = dist_from_center(img, leftx, rightx, xmtr_per_pixel, ymtr_per_pixel)\n",
    "    \n",
    "    out_img = np.copy(img)\n",
    "    avg_rad = round(np.mean([left_curvature, right_curvature]),0)\n",
    "    cv2.putText(out_img, 'Average lane curvature: {:.2f} m'.format(avg_rad), \n",
    "                (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv2.putText(out_img, dist_txt, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    \n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:    \n",
    "    img = undistort(image, objpoints, imgpoints)\n",
    "    \n",
    "    combined_binary = get_bin_img(img, kernel_size=kernel_size, sobel_thresh=mag_thresh, r_thresh=r_thresh, \n",
    "                                  s_thresh=s_thresh, b_thresh = b_thresh, g_thresh=g_thresh)\n",
    "    warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary)\n",
    "    \n",
    "    xmtr_per_pixel=3.7/800\n",
    "    ymtr_per_pixel=30/720\n",
    "    \n",
    "    left_fit, right_fit, left_fitx, right_fitx, left_lane_indices, right_lane_indices, out_img = fit_polynomial(warped, nwindows=12, show=False)\n",
    "    lane_img = draw_lines(img, left_fit, right_fit, minv)\n",
    "    out_img = show_curvatures(lane_img, left_fit, right_fit, xmtr_per_pixel, ymtr_per_pixel)\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,8))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(\"Original:: \" + image , fontsize=18)\n",
    "    ax2.imshow(cv2.cvtColor(out_img, cv2.COLOR_BGR2RGB))\n",
    "    ax2.set_title(\"Lane:: \"+ image, fontsize=18)\n",
    "    f.savefig(OUTDIR + \"/op_\" + str(time.time()) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lane():\n",
    "    def __init__(self, max_counter):\n",
    "        self.current_fit_left=None\n",
    "        self.best_fit_left = None\n",
    "        self.history_left = [np.array([False])] \n",
    "        self.current_fit_right=None\n",
    "        self.best_fit_right = None\n",
    "        self.history_right = [np.array([False])] \n",
    "        self.counter = 0\n",
    "        self.max_counter = 1\n",
    "        self.src = None\n",
    "        self.dst = None\n",
    "        \n",
    "    def set_presp_indices(self, src, dest):\n",
    "        self.src = src\n",
    "        self.dst = dst\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_fit_left=None\n",
    "        self.best_fit_left = None\n",
    "        self.history_left =[np.array([False])] \n",
    "        self.current_fit_right = None\n",
    "        self.best_fit_right = None\n",
    "        self.history_right =[np.array([False])] \n",
    "        self.counter = 0\n",
    "        \n",
    "    def update_fit(self, left_fit, right_fit):\n",
    "        if self.counter > self.max_counter:\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.current_fit_left = left_fit\n",
    "            self.current_fit_right = right_fit\n",
    "            self.history_left.append(left_fit)\n",
    "            self.history_right.append(right_fit)\n",
    "            self.history_left = self.history_left[-self.max_counter:] if len(self.history_left) > self.max_counter else self.history_left\n",
    "            self.history_right = self.history_right[-self.max_counter:] if len(self.history_right) > self.max_counter else self.history_right\n",
    "            self.best_fit_left = np.mean(self.history_left, axis=0)\n",
    "            self.best_fit_right = np.mean(self.history_right, axis=0)\n",
    "        \n",
    "    def process_image(self, image):\n",
    "        img = undistort_no_read(image, objpoints, imgpoints)\n",
    "        \n",
    "        combined_binary = get_bin_img(img, kernel_size=kernel_size, sobel_thresh=mag_thresh,\n",
    "                                      r_thresh=r_thresh, s_thresh=s_thresh, b_thresh = b_thresh, g_thresh=g_thresh)\n",
    "    \n",
    "        if self.src is not None or self.dst is not None:\n",
    "            warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary, src=self.src, dst= self.dst)\n",
    "        else:\n",
    "            warped, warp_matrix, unwarp_matrix, out_img_orig, out_warped_img = transform_image(combined_binary)\n",
    "    \n",
    "        xmtr_per_pixel=3.7/800\n",
    "        ymtr_per_pixel=30/720\n",
    "    \n",
    "        if self.best_fit_left is None and self.best_fit_right is None:\n",
    "            left_fit, right_fit, left_fitx, right_fitx, left_lane_indices, right_lane_indices, out_img = fit_polynomial(warped, nwindows=15, show=False)\n",
    "        else:\n",
    "            left_fit, right_fit, left_lane_indices, right_lane_indices= search_around_poly(warped, self.best_fit_left, self.best_fit_right, xmtr_per_pixel, ymtr_per_pixel)\n",
    "            \n",
    "        self.counter += 1\n",
    "        \n",
    "        lane_img = draw_lines(img, left_fit, right_fit, unwarp_matrix)\n",
    "        out_img = show_curvatures(lane_img, left_fit, right_fit, xmtr_per_pixel, ymtr_per_pixel)\n",
    "        \n",
    "        self.update_fit(left_fit, right_fit)\n",
    "        \n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip1 = VideoFileClip(\"project.mp4\")\n",
    "img = clip1.get_frame(0)\n",
    "\n",
    "leftupper  = (585, 460)\n",
    "rightupper = (705, 460)\n",
    "leftlower  = (210, img.shape[0])\n",
    "rightlower = (1080, img.shape[0])\n",
    "    \n",
    "color_r = [255, 0, 0]\n",
    "color_g = [0, 255, 0]\n",
    "line_width = 5\n",
    "    \n",
    "src = np.float32([leftupper, leftlower, rightupper, rightlower])\n",
    "\n",
    "cv2.line(img, leftlower, leftupper, color_r, line_width)\n",
    "cv2.line(img, leftlower, rightlower, color_r , line_width * 2)\n",
    "cv2.line(img, rightupper, rightlower, color_r, line_width)\n",
    "cv2.line(img, rightupper, leftupper, color_g, line_width)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane1 = Lane(max_counter=5)\n",
    "\n",
    "leftupper  = (585, 460)\n",
    "rightupper = (705, 460)\n",
    "leftlower  = (210, img.shape[0])\n",
    "rightlower = (1080, img.shape[0])\n",
    "    \n",
    "warped_leftupper = (250,0)\n",
    "warped_rightupper = (250, img.shape[0])\n",
    "warped_leftlower = (1050, 0)\n",
    "warped_rightlower = (1050, img.shape[0])\n",
    "\n",
    "src = np.float32([leftupper, leftlower, rightupper, rightlower])\n",
    "dst = np.float32([warped_leftupper, warped_rightupper, warped_leftlower, warped_rightlower])\n",
    "\n",
    "lane1.set_presp_indices(src, dst)\n",
    "\n",
    "output = \"test_videos_output/project.mp4\"\n",
    "clip1 = VideoFileClip(output)\n",
    "white_clip = clip1.fl_image(lane1.process_image)\n",
    "%time white_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing on challenge video and harder challenge video, the problems encountered are mostly due to lighting condition, shadows and road conditions ( edge visible on road other than lane marking). Although HLS space works well for simple video, it activates noisy areas more. ( that's visible in video 2 and 3). I may try LAB color space which separates yellow color better.<br/>\n",
    "<br/>\n",
    "The averaging of lane works well to smoothen the polynomial output. Harder challenge also poses a problem with very steep curves too. May be we need to fit higher polynomial to these steep curves<br/>\n",
    "<br/>\n",
    "Also, still these algorithms relie a lot on lane being visible, video being taken from certain angle, light condition and still feels like hand crafted. There might be better way based off RNN(ReNet) or Instance Segmentation (https://arxiv.org/pdf/1802.05591.pdf) or spatial CNN (https://arxiv.org/pdf/1712.06080.pdf) <br/>\n",
    "\n",
    "I wonder what the tesla is using  that they displayed in recent autonomy day video.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "791945ebddc8c7a256e44d685b9a364c892a9da2617cadfc6e2c6642dc28e44d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
